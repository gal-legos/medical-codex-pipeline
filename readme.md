# This is assignment 1. 

The first step was to create the medical codex outline and create the necessary folders and files needed to begin the assignment. This meant creating the 'input' folder where all the original uncleaned data files would live; an 'output' folder for all of the cleaned .csv files; a 'scripts' folder which contained all seven of the python files relevant to the their respective medical codexes; the '.gitignore' file which contains the information of what files can be ignored and not pushed to the github repository; the 'requirements.txt' file which held all the necessary packages; and lastly the 'readme.md' file that has all of the file descriptions and explanations. 

# gitignore
The gitignore basic code was copied and pasted from the assignment 1 requirements located in the assignments folder in the HHA-507-2025 github repository. While working on this assignment changes were made to the gitignore file as necessary and were then saved and pushed to the main branch. 

# hcps data cleaning 
The HCPS data file cleaning began with importing the pandas package in the hcps_processor.py file. From there, I attempted to follow the basic coding structure with the help of the AI helper tool. Using it as a guide, I created a dataframe for the HCPS file. From there, I attempted to remove trailing spaces from the data set by using ' hcps_data['code'] = hcps_data['code'].str.strip() ' and ' hcps_data['description'] = hcps_data['description'].str.strip() '. After deleting the trailing space, I chose to remove any duplicates from the dataset by using 'hcps_data = hcps_data.drop_duplicates()'. This allowed for the data to have unique information to be kept. Once this step was completed, I added a 'last_updated' column since it did not exist in the original data file. Having cleaned the file, I created an output path where the cleaned data file could live in the 'output' folder. After looking at the data, there seemed to be some adjustments that needed to be made as a result of the incorrect spacing of the data and where the data should be separated. This led to problems when I attempted to change the code in order to fix it. Ultimately, I decided to leave the original code as is to avoid potential problems with the data.


# icd10cm data cleaning 
Similar to the HCPS data cleaning, the first step I took was to import pandas into the Python file. This data cleaning was also my attempt at trying to do it on my own, with some help using the AI helper. I mainly used the basic code structure from the hcps python file located in the HHA-507-2025 repo, as a guide and the zoom lecture videos. Now having the proper packages, I was able to have the pandas package read the data and turn it into a dataframe using the 'df = pd.read_csv()' function. Within that dataframe, it was also specified which delimiter was present in the original data set, if there was a header or not, and which columns were going to be created in the dataframe. Seeing as there wasn't a 'last_updated' column in the data set, I created one using ' df['last_updated'] = '2025-09-01' '. With all 3 columns created and named, I then created a file output path and ran the script.


# icd102019 data cleaning 
To begin cleaning the icd102019 data, the basic structure code from the "icd_who.py" file was copied and pasted into my local icd10who_processor.py file. The basic code structure ensured that the column names, which were nonexistent in the original icd102019syst_codes.txt file, were included. Once pasted into my local Python file, I went ahead and changed the necessary filepaths in the code. Then I proceeded to drop the columns from the dataset that were not relevant to the assignment. For example, the 'code' was kept as it matched the assignment 'code' requirements, and 'title_en' was kept because it was the closest column that had a description of said code. With these two columns meeting the criteria, the rest of the columns were not needed and could be dropped. This was done by using 'df = df.drop(columns=[])' and then was followed by 'df['last_updated'] = '2025-09-01'' to add the last_updated column. Once all three columns were established, they were renamed to follow the assignment standards, and finally, the output filepath was changed to match the local file output location.
 

# loinc data cleaning 
To clean the Loinc dataset, I began by importing pandas to help read through the original file. After using pandas to read the data file, I used 'loinc.info()' as a way to look through the columns and find which ones would be kept and renamed. Then I created a dataframe called 'loinc_small' that was composed of the 'LOINC_NUM' and 'LONG_COMMON_NAME' columns. 'LOINC_NUM' met the assignment criteria since its column contents were most similar to the contents that would reside in a 'code' column. I had similar reasoning for keeping the "LONG_COMMON_NAME" column. In said column, it was the closest to the description that best matched the code column. Still needing the third column to satisfy the assignment requirements, I used 'loinc_small['last_updated'] = '2025-09-01'' to create the last column. After creating the third column, the rest of the columns were renamed to match the assignment instructions by using 'loinc_small = loinc_small.rename(columns={})'. With the columns renamed properly, all that was left to do was create an output path for the cleaned loinc.csv file to exist in. This was done by writing out this code
'file_output_path = 'output/loinc_small.csv';loinc_small.to_csv(file_output_path, index=False)' and then proceeding to run the entire python script.


# npi data cleaning 
To begin cleaning the NPI file properly, I copied the code from the npi.py file in the HHA-507-2025 GitHub repository. Once it was pasted into my local npi_processor.py file, I proceeded to change the input filepath to ' npi_file_path = 'input/npidata_pfile_20050523-20250907.csv' '. This ensures that the script is reading the data from my local file in the input folder. From there I also made more name changes in the code and ran the code to make sure that the columns in the code matched with the columns required by the assingment. The original npidata_pfile_20050523-20250907.txt file was exceptionally large (it was about 10 GB), and made it difficult to load on either my computer as a txt file or on vscode. As a result I was hoping and trusting that the python script would run and give me exactly what I needed. Utilizing the original basic code also helped by giving me an idea of which columns I would be seeing in the terminal and could organize properly. In addition to changing out the filepath names, I created a sample size of the cleaned npi file to push to github since the npi_small.csv file would have been too large to push directly into GitHub and would have likely come up as an error. 


# rxnorm data cleaning 
To clean the RXNORM file, I began by copying and pasting the basic code structure from the rx_norm.py file in the HHA-507-2025 repo. I left the import polars as pl function because the original data set was too big. Once pasted, I also utilized the RXNORM technical description information from nih website (https://www.nlm.nih.gov/research/umls/rxnorm/docs/techdoc.html#s12_0) to familiarize myself with the columns properly from the script. After reading through the technical descriptions, I was able to figure out which columns to drop and which to keep. The columns kept were 'rxaui', 'str', and 'updated_timestamp'. The first column was kept in the dataframe based on the technical documentation from the URL provided. There, I could see that the data contained in the 'rxaui' column was a unique identifier, as per the website, which was most similar to a code criterion. The 'str' column was kept because, by looking at the dataset, I was able to see that the data in that column was the most similar to a description of the code. Since this file already had a last updated column, there was no need to write a script to include it. Since all the necessary columns were identified, they now needed to be isolated. I did this by dropping the rest of the columns using 'df = df.drop([])'. From here, I renamed the columns to 'code, 'descriptionâ€™, and 'last_updated', respectively, by using the 'df = df.rename({})' function. Once the file output path code was changed to match my local file output, and proceeded to run the script to create the RXNATOMARCHIVE.csv file.


# snomed data cleaning 
The very first step I took in cleaning the original sct2_Description_Full-en_US1000124_20250901.txt file was opening it up on my own computer and combing through to see if there were any headers in the original, and if so, what were the header names. From there, I was able to find the rows with the information I needed. After that step, I went into the HHA-507-2025 repo folder and located the basic Python structure code. I copied and pasted the code from the repo into my own snowmed_processor.py file. Once the code was pasted, I went ahead and made the necessary changes to the code that were relevant to my folder and file names. The original copied code contains scripts that help read the large original data file and prevent the Polars packages from causing errors. It also organized the original data file into a cleaned csv file; however, this file itself is still too large for GitHub and is not trimmed down to only display the required columns for this assignment (e.g., code, description, last updated). From here, columns that do not contain data that fall under or could be considered code, description, and last updated are removed using 'df = df.drop([])'. Once this step is completed, we need to add a 'last updated' column since there isn't one. Using 'df = df.with_columns(pl.lit('2025-09-01').alias('last_updated'))', I was able to add the last updated column, and with no specific date given, we will just assume 09-01-2025 was the last updated date available. With the unnecessary columns removed, and the last updated column added, I used 'df = df.rename({})' to rename the columns from their original name to the required name and used the AI helper to write the proper 'print' functions. After the SNOMED file was completely cleaned and displayed the proper columns and information, the file was still too large for GitHub. In order to make it possible to push these changes to GitHub, I made a small sample of the cleaned file that would be small enough to push through to GitHub but large enough to have a significant sample size. To make the sample, I used 'df.head(10_000).write_csv(output_dir / "snomed_ct_cleaned_sample.csv")' and updated my .gitignore file to include 'outpit/snomed_ct_cleaned.csv'. With these changes made, I was able to successfully commit these changes to my GitHub repository. 